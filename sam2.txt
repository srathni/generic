The Spark ETL framework is designed with robust mechanisms to ensure any failure is properly captured, logged, and escalated to the appropriate support teams for resolution. The current failure handling strategy focuses on batch-level fail-safety with future provisions for granular control.

1. Capturable Failures (Business Logic / Data Errors)

If a rule fails during execution and the error can be captured (e.g., bad SQL, missing columns, logical inconsistencies), the framework:
Fails the entire batch immediately.
Logs the exception details in the server logs.
Publishes a structured error message to Kafka for monitoring and metrics dashboards.
Causes the associated AutoSys job to fail.
The Level 2 (L2) operations team is alerted upon job failure. They:
Review the logged error.
Restart the batch if the issue is transient or resolvable without code changes.
Escalate the issue to the development team if deeper investigation or fixes are needed.
2. System-Level Failures (Cluster Crash, Spark Driver Failure, etc.)

In the case of failures that cannot be captured due to system-level issues:
The L2 team monitors the batch failure through AutoSys or infrastructure alerts.
They alert the development team for investigation and guidance.
The team will determine whether a rerun is safe or if manual intervention is required.
3. Pre-Batch Cleanup

Prior to starting any batch, the framework performs cleanup of target output directories.
This ensures the system maintains delete-and-load semantics, avoiding duplicates or conflicts in case of re-runs.
Prevents partial data residues in case of mid-batch failures.
4. Resume and Retry Strategy

Currently, the framework does not support automatic rule-level resume or partial retries.
The focus for the MVP is to ensure reliability at the batch level first.
Rule-level resume capabilities are planned for a future enhancement, which would include:
Rule execution state checkpoints.
Retry flags per rule.
Dependency-aware rule execution tracking.
