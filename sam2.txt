Monitoring & Observability
The Spark-based ETL framework is built with robust monitoring and observability mechanisms to ensure traceability, performance tracking, and reliable failure detection.

1. Kafka-Based Logging

Each rule execution within a batch emits structured logs to a Kafka topic. These logs include:

Rule ID and Batch ID
Start and End Timestamps
Input and Output Record Counts
Execution Status (Success/Failure)
Exception Stack Trace (if any)
These Kafka logs are consumed by downstream systems for analysis, alerting, and metrics aggregation.

2. Server Logging

In addition to Kafka-based logging, all Spark framework logs are persisted to local server log directories. These include:

Batch-level and Rule-level execution logs
Spark driver and executor logs
SQL translation and execution diagnostics
Exception and error stack traces
DevOps-defined log rotation and archival policies are applied to manage disk usage.

3. Failure Notification & Alerting

Any rule failure due to logical or data-related errors is captured and logged.
The associated batch is marked as failed, and the Autosys job is marked unsuccessful.
L2 Support is alerted through monitoring tools.
For recoverable issues, L2 can restart the batch. Complex issues are escalated to the development team.
4. Real-Time Dashboards & API Integration

The framework integrates with an existing in-house API and monitoring dashboard.
All rule and batch-level metrics, including duration, record counts, and statuses, are sent in real time.
This allows stakeholders to view execution summaries, track SLAs, and monitor anomalies without direct access to Spark or logs.
