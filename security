# Import necessary libraries
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType
import os

# --- PySpark Session Configuration ---
# To run this code, you'll need to submit it with the following packages:
# org.apache.iceberg:iceberg-spark-runtime-3.x_2.12 (replace 3.x with your Spark major version, e.g., 3.5)
# org.apache.hadoop:hadoop-aws:3.3.2 (or compatible version with your Hadoop/Spark)
# com.amazonaws:aws-java-sdk-bundle:1.12.262 (or compatible version)

# Example spark-submit command (adjust versions as needed):
# spark-submit \
#   --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
#   --conf "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions" \
#   --conf "spark.sql.catalog.s3_catalog=org.apache.iceberg.spark.SparkCatalog" \
#   --conf "spark.sql.catalog.s3_catalog.type=hadoop" \
#   --conf "spark.sql.catalog.s3_catalog.warehouse=s3a://your-bucket-name/iceberg_warehouse" \
#   --conf "spark.hadoop.fs.s3a.endpoint=http://localhost:9000" \
#   --conf "spark.hadoop.fs.s3a.access.key=YOUR_ACCESS_KEY" \
#   --conf "spark.hadoop.fs.s3a.secret.key=YOUR_SECRET_KEY" \
#   --conf "spark.hadoop.fs.s3a.path.style.access=true" \
#   --conf "spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem" \
#   your_script_name.py

# Initialize SparkSession
# Replace 'your-bucket-name' with your actual S3 bucket name
# Replace 'http://localhost:9000' with your S3-compatible object store endpoint (e.g., MinIO, Ceph, etc.)
# Replace 'YOUR_ACCESS_KEY' and 'YOUR_SECRET_KEY' with your actual credentials
# Ensure 'spark.hadoop.fs.s3a.path.style.access=true' is set for S3-compatible stores that require it.

spark = SparkSession.builder \
    .appName("IcebergS3Example") \
    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
    .config("spark.sql.catalog.s3_catalog", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.s3_catalog.type", "hadoop") \
    .config("spark.sql.catalog.s3_catalog.warehouse", "s3a://your-bucket-name/iceberg_warehouse") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "YOUR_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "YOUR_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()

# Set log level to WARN to reduce verbosity
spark.sparkContext.setLogLevel("WARN")

print("SparkSession created successfully with Iceberg and S3 configurations.")

# --- Define Schema for the Iceberg Table ---
schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("name", StringType(), True),
    StructField("city", StringType(), True),
    StructField("age", IntegerType(), True),
    StructField("salary", DoubleType(), True)
])

# --- Create Sample Data ---
data = [
    (1, "Alice", "New York", 30, 75000.0),
    (2, "Bob", "Los Angeles", 24, 60000.50),
    (3, "Charlie", "Chicago", 35, 90000.75),
    (4, "David", "Houston", 29, 68000.0),
    (5, "Eve", "Miami", 42, 105000.25)
]

# Create a DataFrame from the sample data and schema
df = spark.createDataFrame(data, schema)
print("\nSample DataFrame created:")
df.show()
df.printSchema()

# --- Create and Write to Iceberg Table ---
table_name = "s3_catalog.default.my_iceberg_table"
print(f"\nAttempting to write DataFrame to Iceberg table: {table_name}")

try:
    # Write the DataFrame to the Iceberg table
    # 'append' mode will add new rows, 'overwrite' will replace the table content
    # 'createOrReplace' will create if not exists, or replace if exists
    df.writeTo(table_name).createOrReplace()
    print(f"Successfully wrote data to Iceberg table: {table_name}")
except Exception as e:
    print(f"Error writing to Iceberg table: {e}")
    # If the table already exists and you want to append:
    # df.writeTo(table_name).append()
    # If the table already exists and you want to overwrite:
    # df.writeTo(table_name).overwritePartitions() # Use if partitioned
    # df.writeTo(table_name).mode("overwrite").save() # General overwrite

# --- Read Data from Iceberg Table (for verification) ---
print(f"\nAttempting to read data from Iceberg table: {table_name}")
try:
    read_df = spark.read.table(table_name)
    print("Data read from Iceberg table:")
    read_df.show()
    read_df.printSchema()
except Exception as e:
    print(f"Error reading from Iceberg table: {e}")

# --- Stop SparkSession ---
spark.stop()
print("\nSparkSession stopped.")
