from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Initialize Spark session
spark = SparkSession.builder \
    .appName("PivotExample") \
    .getOrCreate()

# Method 1: Create DataFrame from list of tuples
print("=== Method 1: Creating DataFrame from tuples ===")
data = [
    ("2023", "Jan", "Electronics", 1000),
    ("2023", "Jan", "Clothing", 800),
    ("2023", "Feb", "Electronics", 1200),
    ("2023", "Feb", "Clothing", 900),
    ("2023", "Mar", "Electronics", 1100),
    ("2023", "Mar", "Clothing", 850),
    ("2024", "Jan", "Electronics", 1300),
    ("2024", "Jan", "Clothing", 950),
    ("2024", "Feb", "Electronics", 1400),
    ("2024", "Feb", "Clothing", 1000),
    ("2024", "Mar", "Electronics", 1250),
    ("2024", "Mar", "Clothing", 1100)
]

columns = ["year", "month", "category", "sales"]
df = spark.createDataFrame(data, columns)

print("Original DataFrame:")
df.show()

# Method 2: Create DataFrame with explicit schema
print("\n=== Method 2: Creating DataFrame with explicit schema ===")
schema = StructType([
    StructField("year", StringType(), True),
    StructField("month", StringType(), True),
    StructField("category", StringType(), True),
    StructField("sales", IntegerType(), True)
])

df_with_schema = spark.createDataFrame(data, schema)
print("DataFrame created with explicit schema:")
df_with_schema.printSchema()

# Register as temporary view for SQL operations
df.createOrReplaceTempView("sales_data")

print("\n=== PIVOT Examples ===")

# Example 1: Pivot by month, sum sales for each category
print("1. Pivot by month (columns) for each category:")
pivot_query1 = """
SELECT * FROM (
    SELECT year, month, category, sales 
    FROM sales_data
) PIVOT (
    SUM(sales)
    FOR month IN ('Jan', 'Feb', 'Mar')
)
ORDER BY year, category
"""

result1 = spark.sql(pivot_query1)
result1.show()

# Example 2: Pivot by category, sum sales for each year-month combination
print("2. Pivot by category (columns) for each year-month:")
pivot_query2 = """
SELECT * FROM (
    SELECT year, month, category, sales 
    FROM sales_data
) PIVOT (
    SUM(sales)
    FOR category IN ('Electronics', 'Clothing')
)
ORDER BY year, month
"""

result2 = spark.sql(pivot_query2)
result2.show()

# Example 3: Multiple aggregations in pivot
print("3. Pivot with multiple aggregations:")
pivot_query3 = """
SELECT * FROM (
    SELECT year, month, category, sales 
    FROM sales_data
) PIVOT (
    SUM(sales) as total_sales,
    AVG(sales) as avg_sales,
    COUNT(sales) as count_sales
    FOR month IN ('Jan', 'Feb', 'Mar')
)
ORDER BY year, category
"""

result3 = spark.sql(pivot_query3)
result3.show()

# Example 4: Using DataFrame API (alternative to SQL)
print("4. Using DataFrame API pivot:")
df_pivot = df.groupBy("year", "category") \
    .pivot("month", ["Jan", "Feb", "Mar"]) \
    .sum("sales") \
    .orderBy("year", "category")

df_pivot.show()

# Example 5: Pivot with filtering
print("5. Pivot with filtering (only 2023 data):")
pivot_query4 = """
SELECT * FROM (
    SELECT year, month, category, sales 
    FROM sales_data
    WHERE year = '2023'
) PIVOT (
    SUM(sales)
    FOR month IN ('Jan', 'Feb', 'Mar')
)
ORDER BY category
"""

result4 = spark.sql(pivot_query4)
result4.show()

# Clean up
spark.stop()

print("\n=== Key Points ===")
print("1. PIVOT transforms rows into columns")
print("2. You must specify exact values in the IN clause")
print("3. Common aggregations: SUM, COUNT, AVG, MAX, MIN")
print("4. Can use multiple aggregations in a single PIVOT")
print("5. Available via both SQL and DataFrame API")
print("6. NULL values appear where no data exists")
